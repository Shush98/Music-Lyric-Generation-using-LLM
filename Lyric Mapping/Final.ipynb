{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42158e58-c18e-415a-aa44-2be027b4167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bba0d67-3fda-44bb-a014-d7d7bf69834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12e8f478-51b6-405c-b6e7-f5e28f7d576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a7af06a-4edc-4ab1-9f25-66f45735df74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Genre</th>\n",
       "      <th>Lyric_cleaned</th>\n",
       "      <th>target_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Let there be lions\\nLet them free me from my skin\\nLet there be water\\nI wanna drown and touch the ground\\nAs I drink the water of this earth\\nAs I breathe the air in front of meMISSINGSeek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sun</td>\n",
       "      <td>Live in fear\\nLive in fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Live in fear\\nLive in fearMISSINGThis here is my solitude\\nIt stays with me\\nMy source of fortitude</td>\n",
       "      <td>Seek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metal</td>\n",
       "      <td>Seek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sunMISSINGI have seen through the Hyenas eyes</td>\n",
       "      <td>This here is my solitude\\nIt stays with me\\nMy source of fortitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Metal</td>\n",
       "      <td>This here is my solitude\\nIt stays with me\\nMy source of fortitudeMISSINGHigh I wanna fly\\nTo spread my wings fall from the sky\\nDeep I wanna dive\\nTo give in to the depths to feel alive\\nLike a storm Im comin over you\\nLike a wave take you away</td>\n",
       "      <td>I have seen through the Hyenas eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Metal</td>\n",
       "      <td>I have seen through the Hyenas eyesMISSINGLive in fear\\nLive in fear</td>\n",
       "      <td>High I wanna fly\\nTo spread my wings fall from the sky\\nDeep I wanna dive\\nTo give in to the depths to feel alive\\nLike a storm Im comin over you\\nLike a wave take you away</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Genre  \\\n",
       "0  Metal   \n",
       "1  Metal   \n",
       "2  Metal   \n",
       "3  Metal   \n",
       "4  Metal   \n",
       "\n",
       "                                                                                                                                                                                                                                                                              Lyric_cleaned  \\\n",
       "0  Let there be lions\\nLet them free me from my skin\\nLet there be water\\nI wanna drown and touch the ground\\nAs I drink the water of this earth\\nAs I breathe the air in front of meMISSINGSeek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sun   \n",
       "1                                                                                                                                                                                       Live in fear\\nLive in fearMISSINGThis here is my solitude\\nIt stays with me\\nMy source of fortitude   \n",
       "2                                                                                                                                                 Seek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sunMISSINGI have seen through the Hyenas eyes   \n",
       "3                                     This here is my solitude\\nIt stays with me\\nMy source of fortitudeMISSINGHigh I wanna fly\\nTo spread my wings fall from the sky\\nDeep I wanna dive\\nTo give in to the depths to feel alive\\nLike a storm Im comin over you\\nLike a wave take you away   \n",
       "4                                                                                                                                                                                                                      I have seen through the Hyenas eyesMISSINGLive in fear\\nLive in fear   \n",
       "\n",
       "                                                                                                                                                                 target_cleaned  \n",
       "0                                                                                                                                                    Live in fear\\nLive in fear  \n",
       "1                                                                               Seek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sun  \n",
       "2                                                                                                            This here is my solitude\\nIt stays with me\\nMy source of fortitude  \n",
       "3                                                                                                                                           I have seen through the Hyenas eyes  \n",
       "4  High I wanna fly\\nTo spread my wings fall from the sky\\nDeep I wanna dive\\nTo give in to the depths to feel alive\\nLike a storm Im comin over you\\nLike a wave take you away  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff3998f-9c69-48bb-b2c4-51373166070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389f4bf5-cb80-4bb4-8ac7-547a0b07baf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Genre             0\n",
       "Lyric_cleaned     0\n",
       "target_cleaned    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bd9135-46b8-4505-9e4b-c0b9cad1b5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee319fd-f2e8-4c67-99ab-df93166f04fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "accelerate                         0.29.2\n",
      "aiohttp                            3.9.3\n",
      "aiosignal                          1.3.1\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 2.0.3\n",
      "anaconda-project                   0.9.1\n",
      "anyio                              2.2.0\n",
      "appdirs                            1.4.4\n",
      "argh                               0.26.2\n",
      "argon2-cffi                        20.1.0\n",
      "asn1crypto                         1.4.0\n",
      "astroid                            2.5\n",
      "astropy                            4.2.1\n",
      "async-generator                    1.10\n",
      "async-timeout                      4.0.3\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              20.3.0\n",
      "autopep8                           1.5.6\n",
      "Babel                              2.9.0\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.4\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "beautifulsoup4                     4.9.3\n",
      "bitarray                           2.1.0\n",
      "bitsandbytes                       0.42.0\n",
      "bitsandbytes-cuda112               0.26.0.post2\n",
      "bitsandbytes-cuda113               0.26.0.post2\n",
      "bkcharts                           0.2\n",
      "black                              19.10b0\n",
      "bleach                             3.3.0\n",
      "bokeh                              2.3.2\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "brotlipy                           0.7.0\n",
      "certifi                            2020.12.5\n",
      "cffi                               1.14.5\n",
      "chardet                            4.0.0\n",
      "click                              7.1.2\n",
      "cloudpickle                        1.6.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.4\n",
      "conda                              4.10.1\n",
      "conda-build                        3.21.4\n",
      "conda-content-trust                0+unknown\n",
      "conda-package-handling             1.7.3\n",
      "conda-repo-cli                     1.0.4\n",
      "conda-token                        0.3.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cryptography                       3.4.7\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.23\n",
      "cytoolz                            0.11.0\n",
      "dask                               2021.4.0\n",
      "datasets                           2.18.0\n",
      "decorator                          5.0.6\n",
      "defusedxml                         0.7.1\n",
      "diff-match-patch                   20200713\n",
      "dill                               0.3.8\n",
      "distributed                        2021.4.1\n",
      "docstring_parser                   0.16\n",
      "docutils                           0.17.1\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "eval-type-backport                 0.1.3\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.9.0\n",
      "Flask                              1.1.2\n",
      "frozenlist                         1.4.1\n",
      "fsspec                             2024.2.0\n",
      "future                             0.18.2\n",
      "gevent                             21.1.2\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "greenlet                           1.0.0\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "huggingface-hub                    0.22.2\n",
      "idna                               2.10\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "importlib-metadata                 3.10.0\n",
      "iniconfig                          1.1.1\n",
      "intervaltree                       3.1.0\n",
      "ipykernel                          5.3.4\n",
      "ipython                            7.22.0\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.6.3\n",
      "isort                              5.8.0\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.17.2\n",
      "jeepney                            0.6.0\n",
      "Jinja2                             2.11.3\n",
      "joblib                             1.0.1\n",
      "json5                              0.9.5\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     6.1.12\n",
      "jupyter-console                    6.4.0\n",
      "jupyter-core                       4.7.1\n",
      "jupyter-packaging                  0.7.12\n",
      "jupyter-server                     1.4.1\n",
      "jupyterlab                         3.0.14\n",
      "jupyterlab-pygments                0.1.2\n",
      "jupyterlab-server                  2.4.0\n",
      "jupyterlab-widgets                 1.0.0\n",
      "keyring                            22.3.0\n",
      "kiwisolver                         1.3.1\n",
      "lazy-object-proxy                  1.6.0\n",
      "libarchive-c                       2.9\n",
      "llvmlite                           0.36.0\n",
      "locket                             0.2.1\n",
      "lxml                               4.6.3\n",
      "markdown-it-py                     3.0.0\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.3.4\n",
      "mccabe                             0.6.1\n",
      "mdurl                              0.1.2\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.3.0\n",
      "mkl-random                         1.2.1\n",
      "mkl-service                        2.3.0\n",
      "mock                               4.0.3\n",
      "more-itertools                     8.7.0\n",
      "mpmath                             1.2.1\n",
      "msgpack                            1.0.2\n",
      "multidict                          6.0.5\n",
      "multipledispatch                   0.6.0\n",
      "multiprocess                       0.70.16\n",
      "mypy-extensions                    0.4.3\n",
      "navigator-updater                  0.2.1\n",
      "nbclassic                          0.2.6\n",
      "nbclient                           0.5.3\n",
      "nbconvert                          6.0.7\n",
      "nbformat                           5.1.3\n",
      "nest-asyncio                       1.5.1\n",
      "networkx                           2.5\n",
      "nltk                               3.6.1\n",
      "nose                               1.3.7\n",
      "notebook                           6.3.0\n",
      "numba                              0.53.1\n",
      "numexpr                            2.7.3\n",
      "numpy                              1.22.4\n",
      "numpydoc                           1.1.0\n",
      "nvidia-cublas-cu12                 12.1.3.1\n",
      "nvidia-cuda-cupti-cu12             12.1.105\n",
      "nvidia-cuda-nvrtc-cu12             12.1.105\n",
      "nvidia-cuda-runtime-cu12           12.1.105\n",
      "nvidia-cudnn-cu12                  8.9.2.26\n",
      "nvidia-cufft-cu12                  11.0.2.54\n",
      "nvidia-curand-cu12                 10.3.2.106\n",
      "nvidia-cusolver-cu12               11.4.5.107\n",
      "nvidia-cusparse-cu12               12.1.0.106\n",
      "nvidia-nccl-cu12                   2.19.3\n",
      "nvidia-nvjitlink-cu12              12.4.127\n",
      "nvidia-nvtx-cu12                   12.1.105\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.7\n",
      "packaging                          20.9\n",
      "pandas                             1.2.4\n",
      "pandocfilters                      1.4.3\n",
      "parso                              0.7.0\n",
      "partd                              1.2.0\n",
      "path                               15.1.2\n",
      "pathlib2                           2.3.5\n",
      "pathspec                           0.7.0\n",
      "patsy                              0.5.1\n",
      "peft                               0.10.1.dev0\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             8.2.0\n",
      "pip                                24.0\n",
      "pkginfo                            1.7.0\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "prometheus-client                  0.10.1\n",
      "prompt-toolkit                     3.0.17\n",
      "psutil                             5.8.0\n",
      "ptyprocess                         0.7.0\n",
      "py                                 1.10.0\n",
      "pyarrow                            15.0.2\n",
      "pyarrow-hotfix                     0.6\n",
      "pycodestyle                        2.6.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.20\n",
      "pycurl                             7.43.0.6\n",
      "pydocstyle                         6.0.0\n",
      "pyerfa                             1.7.3\n",
      "pyflakes                           2.2.0\n",
      "Pygments                           2.17.2\n",
      "pylint                             2.7.4\n",
      "pyls-black                         0.4.6\n",
      "pyls-spyder                        0.3.2\n",
      "pyodbc                             5.1.0\n",
      "pyOpenSSL                          20.0.1\n",
      "pyparsing                          2.4.7\n",
      "pyrsistent                         0.17.3\n",
      "PySocks                            1.7.1\n",
      "pytest                             6.2.3\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.4.0\n",
      "python-language-server             0.36.2\n",
      "pytz                               2021.1\n",
      "PyWavelets                         1.1.1\n",
      "pyxdg                              0.27\n",
      "PyYAML                             5.4.1\n",
      "pyzmq                              20.0.0\n",
      "QDarkStyle                         2.8.1\n",
      "QtAwesome                          1.0.2\n",
      "qtconsole                          5.0.3\n",
      "QtPy                               1.9.0\n",
      "regex                              2021.4.4\n",
      "requests                           2.25.1\n",
      "rich                               13.7.1\n",
      "rope                               0.18.0\n",
      "Rtree                              0.9.7\n",
      "ruamel-yaml-conda                  0.15.100\n",
      "safetensors                        0.4.2\n",
      "scikit-image                       0.18.1\n",
      "scikit-learn                       0.24.1\n",
      "scipy                              1.6.2\n",
      "seaborn                            0.11.1\n",
      "SecretStorage                      3.3.1\n",
      "Send2Trash                         1.5.0\n",
      "sentencepiece                      0.2.0\n",
      "setuptools                         52.0.0.post20210125\n",
      "shtab                              1.7.1\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     0.0.0\n",
      "sip                                4.19.13\n",
      "six                                1.15.0\n",
      "sniffio                            1.2.0\n",
      "snowballstemmer                    2.1.0\n",
      "sortedcollections                  2.1.0\n",
      "sortedcontainers                   2.3.0\n",
      "soupsieve                          2.2.1\n",
      "Sphinx                             4.0.1\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             1.0.3\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.4\n",
      "sphinxcontrib-websupport           1.2.4\n",
      "spyder                             4.2.5\n",
      "spyder-kernels                     1.10.2\n",
      "SQLAlchemy                         1.4.15\n",
      "statsmodels                        0.12.2\n",
      "sympy                              1.8\n",
      "tables                             3.6.1\n",
      "tblib                              1.7.0\n",
      "terminado                          0.9.4\n",
      "testpath                           0.4.4\n",
      "textdistance                       4.2.1\n",
      "threadpoolctl                      2.1.0\n",
      "three-merge                        0.1.1\n",
      "tifffile                           2020.10.1\n",
      "tokenizers                         0.15.2\n",
      "toml                               0.10.2\n",
      "toolz                              0.11.1\n",
      "torch                              2.2.2\n",
      "tornado                            6.1\n",
      "tqdm                               4.66.2\n",
      "traitlets                          5.0.5\n",
      "transformers                       4.39.3\n",
      "triton                             2.2.0\n",
      "trl                                0.8.1\n",
      "typed-ast                          1.4.2\n",
      "typing_extensions                  4.11.0\n",
      "tyro                               0.8.3\n",
      "ujson                              4.0.2\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.26.4\n",
      "watchdog                           1.0.2\n",
      "wcwidth                            0.2.5\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           1.0.1\n",
      "wheel                              0.36.2\n",
      "widgetsnbextension                 3.5.1\n",
      "wrapt                              1.12.1\n",
      "wurlitzer                          2.1.0\n",
      "xlrd                               2.0.1\n",
      "XlsxWriter                         1.3.8\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "xxhash                             3.4.1\n",
      "yapf                               0.31.0\n",
      "yarl                               1.9.4\n",
      "zict                               2.0.0\n",
      "zipp                               3.4.1\n",
      "zope.event                         4.5.0\n",
      "zope.interface                     5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e981adce-6cf8-484f-898e-6fcf4f72933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset,test_dataset = train_test_split(data,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "059078a2-a694-46fc-8982-b713f0e8d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.reset_index(inplace=True)\n",
    "test_dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea6f9edd-a8c5-4f41-a166-49a29f17c4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((179962, 5), (19996, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape,test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ba7ca92-053b-43e2-bc76-db5ce2ada6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[['Genre', 'Lyric_cleaned', 'target_cleaned']]\n",
    "test_dataset = test_dataset[['Genre', 'Lyric_cleaned', 'target_cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd48b2f9-6613-4f12-9de6-2af5adde627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Genre', 'Lyric_cleaned', 'target_cleaned'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ada8959c-5d08-4a3c-ab94-2a60cbc033ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# temp_train_df = keyword_train_df[['Lyric', 'genre', 'keywords']]\n",
    "# temp_test_df = keyword_test_df[['Lyric', 'genre', 'keywords']]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset)\n",
    "test_dataset = Dataset.from_pandas(test_dataset)\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "# dataset = load_dataset(\"D3STRON/music_lyrics_5k\")\n",
    "\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM , AutoTokenizer\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import AutoConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "#from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "#Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "#PEFT\n",
    "from peft import LoraConfig\n",
    "from peft import PeftConfig\n",
    "from peft import PeftModel, PeftModel\n",
    "from peft import get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f631d0b1-a9df-471e-b12a-f89ee8c6f783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Genre', 'Lyric_cleaned', 'target_cleaned'],\n",
       "        num_rows: 179962\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Genre', 'Lyric_cleaned', 'target_cleaned'],\n",
       "        num_rows: 19996\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab96fce9-174e-4a32-80e4-de42ba957e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc01c652224d45eebf6ff76d9bc11811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "735ee07d-d8ff-4310-acca-38165e6bc560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8803de36ea5d4a77985d1e3bc94e13c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"Trainable params: {trainable_params} || All params: {all_param} || Trainable %: {100 * trainable_params / all_param}\")\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model_name = \"microsoft/phi-2\"\n",
    "# tokenizer_name = \"microsoft/phi-2\"\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# model_name = \"D3STRON/multi_genre_music_generator\"\n",
    "# tokenizer_name = \"D3STRON/multi_genre_music_generator\"\n",
    "\n",
    "#Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, #4bit quantizaition - load_in_4bit is used to load models in 4-bit quantization \n",
    "    bnb_4bit_use_double_quant=True, #nested quantization technique for even greater memory efficiency without sacrificing performance. This technique has proven beneficial, especially when fine-tuning large models\n",
    "    bnb_4bit_quant_type=\"nf4\", #quantization type used is 4 bit Normal Float Quantization- The NF4 data type is designed for weights initialized using a normal distribution\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #modify the data type used during computation. This can result in speed improvements. \n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=\"auto\",\n",
    "                                                      trust_remote_code=True, \n",
    "                                                      quantization_config=bnb_config\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# config_name = \"ShushantLLM/LLAMA_lyric_generator\"\n",
    "# config = PeftConfig.from_pretrained(config_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config_name)\n",
    "\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#   if \"lora\" in name:  # Adjust the keyword based on your LoRA layer naming convention\n",
    "#     param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5db1f5-4102-44e5-8381-461abcab16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 262410240 || All params: 3500412928 || Trainable %: 7.496550989769399\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ce5236-5005-4c7d-880e-ba384667f620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 40554496 || All params: 3540967424 || Trainable %: 1.1452942414869276\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\n",
    "# Enable gradient checkpointing for the model. Gradient checkpointing is a technique used to reduce the memory consumption during the backward pas. Instead of storing all intermediate activations in the forward pass (which is what's typically done to compute gradients in the backward pass), gradient checkpointing stores only a subset of them\n",
    "model.gradient_checkpointing_enable() \n",
    "\n",
    "# Prepare the model for k-bit training . Applies some preprocessing to the model to prepare it for training.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# If targeting all linear layers\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "\n",
    "    \n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules = target_modules,\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "\n",
    "\n",
    "# Print the number of trainable parameters in the model\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "274bb3cc-207e-407e-ab10-a707c1b56b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 4\n",
    "output_dir = f\"LLama_missing_music_generator\"\n",
    "per_device_train_batch_size = batch_size\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'adamw_hf' #\"paged_adamw_32bit\" #\"paged_adamw_8bit\"\n",
    "save_steps = 10\n",
    "save_total_limit=3\n",
    "logging_steps = 10\n",
    "learning_rate = 1e-5\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.04\n",
    "lr_scheduler_type = 'constant_with_warmup'#\"cosine\"\n",
    "epochs=1\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    num_train_epochs=epochs, \n",
    "    save_steps=save_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    fp16=True,\n",
    "    save_strategy='steps',\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2269984-6e5f-43a4-8ef6-9c7481b25417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420fc76d4ade417f982e999daaa4a78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### USER: Generate missing lyrics of : {example['Lyric_cleaned']} \\n### ASSISTANT: {example['target_cleaned']}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    packing=True,\n",
    "    #dataset_text_field=\"id\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=2048,\n",
    "    formatting_func=formatting_func,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "afa80aab-5d6b-422f-95a6-123e18147a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 5:29:39, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.757300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.610900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.470100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.468500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.418800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.361400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.320800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.302900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.293400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.246400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.271400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.242700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.283500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.280500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.229900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.299700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.223000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.268600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.242500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.241600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.213100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1240, training_loss=2.28213635260059, metrics={'train_runtime': 19794.6051, 'train_samples_per_second': 1.003, 'train_steps_per_second': 0.063, 'total_flos': 1.620717140485079e+18, 'train_loss': 2.28213635260059, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86a8a842-80ed-47c0-9581-2368ded15c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ShushantLLM/LLama_missing_music_generator/commit/8ee21909237e281fa8cbead64e5016dea485f871', commit_message='Training complete', commit_description='', oid='8ee21909237e281fa8cbead64e5016dea485f871', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\", tags=\"missing lyric Llama2 Large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a210c38e-8d8c-49be-ab3b-73a980a0bcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9632be4-6c47-4f93-b143-88ca840ad16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### USER: Generate missing lyrics of : Let there be lions\n",
      "Let them free me from my skin\n",
      "Let there be water\n",
      "I wanna drown and touch the ground\n",
      "As I drink the water of this earth\n",
      "As I breathe the air in front of meMISSINGSeek and you may find\n",
      "Reason to hide\n",
      "Among my own kind\n",
      "Let me be\n",
      "Im falling towards the sun \n",
      "### ASSISTANT:\n"
     ]
    }
   ],
   "source": [
    "Lyric = \"Let there be lions\\nLet them free me from my skin\\nLet there be water\\nI wanna drown and touch the ground\\nAs I drink the water of this earth\\nAs I breathe the air in front of meMISSINGSeek and you may find\\nReason to hide\\nAmong my own kind\\nLet me be\\nIm falling towards the sun\"\n",
    "prompt = f\"\"\"### USER: Generate missing lyrics of : {Lyric} \\n### ASSISTANT:\"\"\".strip()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "192ac1ef-e92a-4c21-8bbe-592fdc9f54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe875313-08c5-4e32-aa18-d1acd573ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "def choose_from_top(probs, cur_ids, n=7, no_rep=2\n",
    "):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob /= np.sum(top_prob)  # Normalize\n",
    "\n",
    "    cur_ids_list = cur_ids.squeeze().cpu().numpy().tolist()\n",
    "    for i in range(100):\n",
    "        if i <= 5:\n",
    "            choice = np.random.choice(ind, 1)[0]\n",
    "        else:\n",
    "            choice = np.random.choice(ind, 1, p=top_prob)[0]\n",
    "        n_gram = cur_ids_list[-no_rep + 1 :] + [choice]\n",
    "        if any(cur_ids_list[i : i + no_rep] == n_gram for i in range(len(cur_ids_list) - no_rep + 1)):\n",
    "            continue\n",
    "        return choice\n",
    "    return np.random.choice(ind, 1, p=top_prob)[0]\n",
    "\n",
    "def generate_response(Lyric : str) -> str:\n",
    "    prompt =  f\"\"\"### ### USER: Generate missing lyrics of : {Lyric} \\n### ASSISTANT:\"\"\".strip()\n",
    "    cur_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(50):\n",
    "            outputs = model(cur_ids)\n",
    "            logits = outputs['logits']\n",
    "            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "            if i == 5:\n",
    "                n = 30\n",
    "            else:\n",
    "                n = 3\n",
    "            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), cur_ids, n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "            if next_token_id in tokenizer.encode('<|endoflyric|>'):\n",
    "                joke_finished = True\n",
    "                break\n",
    "            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "#             print(output_text)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35b8165-94a9-4782-9b65-d495c94591ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### ### USER: Generate missing lyrics of : Let there be lions\n",
      "Let them free me from my skin\n",
      "Let there be water\n",
      "I wanna drown and touch the ground\n",
      "As I drink the water of this earth\n",
      "As I breathe the air in front of meMISSINGSeek and you may find\n",
      "Reason to hide\n",
      "Among my own kind\n",
      "Let me be\n",
      "Im falling towards the sun \n",
      "### ASSISTANT: Im not a man\n",
      "Who isnt scared to fall\n",
      "But if Im gon to die I need to feel it all around me</s>sit down and listen to the story\n",
      "sitting in a chair in the corner in the darkest\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(generate_response(Lyric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d9b00-d972-406d-9c60-554c4eb0d9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75419a3-252a-4788-a5c4-b18a1c40ec6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
