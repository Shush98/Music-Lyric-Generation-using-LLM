{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6cebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_linear_schedule_with_warmup, AdamW\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import datasets \n",
    "# from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba719c7",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "308f37cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435a7dab74c44377a8484e9cc25819f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0fc1bb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>See me, ancient one! Dismal Tuat, Nergal unsaf...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Feels like Im covered in lies so turn off the ...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Works of art, painted black Magniloquent, blee...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Into the cage like an animal You must survive ...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Paralysed in pleasure I hear you call Lost my ...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>499995</td>\n",
       "      <td>[Verse 1] I dont want to tell you that its ove...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>499996</td>\n",
       "      <td>I get to thinking sometimes I dont know why I ...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>499997</td>\n",
       "      <td>When I was A little boy around the table athom...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>499998</td>\n",
       "      <td>[Verse 1] Its a junked out joint off a backroa...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>499999</td>\n",
       "      <td>Well I went to church last Sunday So I could s...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0                                              Lyric    genre\n",
       "0                0  See me, ancient one! Dismal Tuat, Nergal unsaf...    Metal\n",
       "1                1  Feels like Im covered in lies so turn off the ...    Metal\n",
       "2                2  Works of art, painted black Magniloquent, blee...    Metal\n",
       "3                3  Into the cage like an animal You must survive ...    Metal\n",
       "4                4  Paralysed in pleasure I hear you call Lost my ...    Metal\n",
       "...            ...                                                ...      ...\n",
       "499995      499995  [Verse 1] I dont want to tell you that its ove...  country\n",
       "499996      499996  I get to thinking sometimes I dont know why I ...  country\n",
       "499997      499997  When I was A little boy around the table athom...  country\n",
       "499998      499998  [Verse 1] Its a junked out joint off a backroa...  country\n",
       "499999      499999  Well I went to church last Sunday So I could s...  country\n",
       "\n",
       "[500000 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(line):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s,.!?[\\]]')\n",
    "    line = pattern.sub('', line)\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    return line\n",
    "\n",
    "# lyrics_train_df = pd.read_csv('./data/lyrics_train.csv')  # No need for index_col=False, as it's False by default\n",
    "# lyrics_train_df['Lyric'] = lyrics_train_df['Lyric'].str.replace(\"\\r\\n\", \" \").str.replace(\"\\r\", \" \").str.replace(\"\\n\", \" \")\n",
    "# lyrics_train_df['Lyric'] = lyrics_train_df['Lyric'].apply(clean_text)\n",
    "# lyrics_train_df.to_csv('./cleaned_train_lyrics.csv', index=False)\n",
    "lyrics_train_df = pd.read_csv('./data/cleaned_train_lyrics.csv')\n",
    "\n",
    "lyrics_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cede3713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>can you hear me call your name Im not far away...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>You say you are so clever You beleive that you...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Walking across these misery plains When all fo...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Fuck you you bitch get out of my head Twisting...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Crashing forth upon the soil Filthy waves gave...</td>\n",
       "      <td>Metal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>49995</td>\n",
       "      <td>[Verse 1] When the sun sinks down and dreams s...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>49996</td>\n",
       "      <td>I watched from the window as she slipped from ...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>49997</td>\n",
       "      <td>Look around, it is never far See who the wound...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>49998</td>\n",
       "      <td>We started arguing on the onramp Of Interstate...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>49999</td>\n",
       "      <td>This must be my lucky day Babys back again Sai...</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Lyric    genre\n",
       "0               0  can you hear me call your name Im not far away...    Metal\n",
       "1               1  You say you are so clever You beleive that you...    Metal\n",
       "2               2  Walking across these misery plains When all fo...    Metal\n",
       "3               3  Fuck you you bitch get out of my head Twisting...    Metal\n",
       "4               4  Crashing forth upon the soil Filthy waves gave...    Metal\n",
       "...           ...                                                ...      ...\n",
       "49995       49995  [Verse 1] When the sun sinks down and dreams s...  country\n",
       "49996       49996  I watched from the window as she slipped from ...  country\n",
       "49997       49997  Look around, it is never far See who the wound...  country\n",
       "49998       49998  We started arguing on the onramp Of Interstate...  country\n",
       "49999       49999  This must be my lucky day Babys back again Sai...  country\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(line):\n",
    "    pattern = re.compile(r'[^a-zA-Z0-9\\s,.!?[\\]]')\n",
    "    line = pattern.sub('', line)\n",
    "    line = re.sub(r'\\s+', ' ', line).strip()\n",
    "    return line\n",
    "\n",
    "# lyrics_test_df = pd.read_csv('./data/lyrics_test.csv')  # No need for index_col=False, as it's False by default\n",
    "# lyrics_test_df['Lyric'] = lyrics_test_df['Lyric'].str.replace(\"\\r\\n\", \" \").str.replace(\"\\r\", \" \").str.replace(\"\\n\", \" \")\n",
    "# lyrics_test_df['Lyric'] = lyrics_test_df['Lyric'].apply(clean_text)\n",
    "# lyrics_test_df.to_csv('./data/cleaned_test_lyrics.csv', index=False)\n",
    "lyrics_test_df = pd.read_csv('./data/cleaned_test_lyrics.csv')\n",
    "\n",
    "lyrics_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9340adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics_df):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.lyric_list = []\n",
    "        self.end_of_text_token = \"<|endoflyric|>\"\n",
    "        \n",
    "        for lyric, genre in tqdm(zip(lyrics_df['Lyric'],lyrics_df['genre'] ), total=len(lyrics_df['genre'])):\n",
    "            lyric_str = f\"LYRIC[{genre.lower()}]:{lyric}{self.end_of_text_token}\"\n",
    "#             lyric_str = f\"LYRIC:{lyric}{self.end_of_text_token}\"\n",
    "            self.lyric_list.append(lyric_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lyric_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyric_list[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fc1582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32fbd859d6e4890bed61cb63d014d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_data = LyricsDataset(lyrics_train_df)\n",
    "test_data = LyricsDataset(lyrics_test_df[lyrics_test_df['genre'] == 'Metal'])\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6282548",
   "metadata": {},
   "source": [
    "# Training Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "267ef1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token':'<|pad|>'\n",
    "                             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c6a5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perp(model, tokenizer, test_loader):\n",
    "    model.eval()\n",
    "    nlls = []\n",
    "    for lyric in tqdm(test_loader):\n",
    "        lyric_tens = tokenizer(lyric, padding=True, truncation= True, return_tensors='pt')['input_ids'].to(device)\n",
    "        target_tens = lyric_tens.clone()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(lyric_tens, labels=target_tens)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            nlls.append(neg_log_likelihood)\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    print(\"Evaluations:\", ppl.item())\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfc7fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-6\n",
    "WARMUP_STEPS = 80000\n",
    "EVAL_STEPS = 100000\n",
    "PRINT_STEPS = 100\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=-1)\n",
    "proc_seq_count = 0\n",
    "\n",
    "\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979cf022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ed1c4d615a49ca93aeba330f6a6ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m started\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,lyric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[1;32m----> 7\u001b[0m     lyric_tens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(lyric_tens, labels \u001b[38;5;241m=\u001b[39m lyric_tens\u001b[38;5;241m.\u001b[39mclone())\n\u001b[0;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;241m/\u001b[39m BATCH_SIZE\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx,lyric in enumerate(tqdm(train_loader)):\n",
    "        lyric_tens = tokenizer(lyric, padding=True, return_tensors='pt', truncation= True)['input_ids'].to(device)\n",
    "        output = model(lyric_tens, labels = lyric_tens.clone())\n",
    "        loss = output['loss']  / BATCH_SIZE\n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + output['loss'].detach().data\n",
    "        \n",
    "        proc_seq_count = proc_seq_count + 1\n",
    "        steps += 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if batch_count == PRINT_STEPS:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0\n",
    "        if steps > EVAL_STEPS:\n",
    "            steps = 0\n",
    "            calc_perp(model, tokenizer, test_loader)\n",
    "            model.push_to_hub(\"multi-genre-mdeium\")\n",
    "            tokenizer.push_to_hub(\"multi-genre-mdeium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b55b6f",
   "metadata": {},
   "source": [
    "# Evaluations Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac37326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe338ac53104292b9533abf47e4b5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8756dc23314b86925e9d5a0c87b920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f9b83310d84998a53020539d2e0d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddc0b25626b44f2a07ad0644e584467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03520cf8d74447db9dcf8b47bedc455c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc173e71e45412889423c92bf8a9f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2405bf0d8046a096610a028a68f8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random \n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "    \n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token':'<|pad|>'\n",
    "                             })\n",
    "model.to(device)\n",
    "\n",
    "def evaluate(model, tokenizer, test_loader):\n",
    "    model.eval()\n",
    "    nlls = []\n",
    "    EVAL_STEPS = 1000\n",
    "    steps = 0\n",
    "    for lyric in tqdm(test_loader):\n",
    "        lyric_tens = tokenizer(lyric, padding=True, truncation= True, return_tensors='pt')['input_ids'].to(device)\n",
    "        target_tens = lyric_tens.clone()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(lyric_tens, labels=target_tens)\n",
    "            neg_log_likelihood = outputs.loss\n",
    "            nlls.append(neg_log_likelihood)\n",
    "        if EVAL_STEPS == steps:\n",
    "            steps = 0\n",
    "            ppl = torch.exp(torch.stack(nlls).mean())\n",
    "            print(\"Evaluations:\", ppl.item())\n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24bd75e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee2f70f5d3c4c7886d76791cc3f2ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluations: 89.34149169921875\n",
      "Evaluations: 91.60322570800781\n",
      "Evaluations: 90.94165802001953\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, tokenizer, test_loader)\u001b[0m\n\u001b[0;32m     24\u001b[0m steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lyric \u001b[38;5;129;01min\u001b[39;00m tqdm(test_loader):\n\u001b[1;32m---> 26\u001b[0m     lyric_tens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlyric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     target_tens \u001b[38;5;241m=\u001b[39m lyric_tens\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    # train_data = LyricsDataset(lyrics_train_df)\n",
    "    test_data = LyricsDataset(lyrics_test_df[lyrics_test_df['genre'] == genre])\n",
    "\n",
    "    # train_loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "    evaluate(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828d43e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1d9420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"D3STRON/multi_genre_music_generator\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"D3STRON/multi_genre_music_generator\")\n",
    "model.to(device)\n",
    "\n",
    "def choose_from_top(probs, cur_ids, n=7, no_rep=2):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob /= np.sum(top_prob)  # Normalize\n",
    "\n",
    "    cur_ids_list = cur_ids.squeeze().cpu().numpy().tolist()\n",
    "    for i in range(200):\n",
    "        if i < 4:\n",
    "            choice = np.random.choice(ind, 1)[0]\n",
    "        else:\n",
    "            choice = np.random.choice(ind, 1, p=top_prob)[0]\n",
    "        n_gram = cur_ids_list[-no_rep + 1 :] + [choice]\n",
    "        if any(cur_ids_list[i : i + no_rep] == n_gram for i in range(len(cur_ids_list) - no_rep + 1)):\n",
    "            continue\n",
    "        return choice\n",
    "    return np.random.choice(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e93529a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LYRIC[pop]:A bundle of joy A promise that could not be fulfilled The world is filled with love and happiness And all that I want for you is a place in the sky But you cant find me on your knees And the sun wont set on my heart But its all right for me And youre a part in a beautiful life And I dont have no shame But Ive never known the way you feel Youre the love that you never knew you needed I know the world has been full for years but youve always been here For\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    cur_ids = tokenizer(\"LYRIC[pop]:A bundle of joy\", padding=True, return_tensors='pt', truncation= True)['input_ids'].to(device)\n",
    "    for i in range(100):\n",
    "        outputs = model(cur_ids, labels=cur_ids)\n",
    "        logits = outputs['logits']\n",
    "        softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "        if i == 3:\n",
    "            n = 30\n",
    "        else:\n",
    "            n = 5\n",
    "        next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), cur_ids, n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "        cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "        if next_token_id in tokenizer.encode('<|endoflyric|>'):\n",
    "            joke_finished = True\n",
    "            break\n",
    "\n",
    "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "    output_text = tokenizer.decode(output_list)\n",
    "    print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc319b",
   "metadata": {},
   "source": [
    "# Evaluation SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d25f9f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from transformers import pipeline\n",
    "import tqdm.auto as tqdm\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a495e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7dc95450ce14bec85952487235e109f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "C:\\Users\\GHOSH\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842f6b0f8803400a896a322bc60873bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4100e6097b924a4d94601b1982ba135f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m)):\n\u001b[0;32m     27\u001b[0m     start_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLYRIC[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 28\u001b[0m     generated_texts \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     generated_texts \u001b[38;5;241m=\u001b[39m [generated_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m generated_text \u001b[38;5;129;01min\u001b[39;00m generated_texts]\n\u001b[0;32m     30\u001b[0m     generations[genre\u001b[38;5;241m.\u001b[39mlower()] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m generated_texts\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:219\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1162\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1155\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1156\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m         )\n\u001b[0;32m   1160\u001b[0m     )\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1169\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1168\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1169\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1170\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py:1068\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1067\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1068\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m   1069\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\text_generation.py:295\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs)\n\u001b[0;32m    296\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1522\u001b[0m     )\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[1;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1526\u001b[0m         input_ids,\n\u001b[0;32m   1527\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   1528\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mlogits_warper,\n\u001b[0;32m   1529\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   1530\u001b[0m         pad_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[0;32m   1531\u001b[0m         eos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[0;32m   1532\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39moutput_scores,\n\u001b[0;32m   1533\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1534\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   1535\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   1536\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1537\u001b[0m     )\n\u001b[0;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[0;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   1549\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:2658\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   2656\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[0;32m   2657\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2658\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2660\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lyrics_test_df = pd.read_csv('./data/cleaned_test_lyrics.csv')\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"D3STRON/multi_genre_music_generator\", device = device)\n",
    "# pipe = pipeline(\"text-generation\", model=\"D3STRON/multi-genre\", device = device)\n",
    "\n",
    "\n",
    "\n",
    "# Generate text\n",
    "\n",
    "\n",
    "generations = {'pop':[],\n",
    "         'rock':[],\n",
    "         'metal':[],\n",
    "         'country':[],\n",
    "         'rap':[]}\n",
    "\n",
    "\n",
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    for batch in tqdm.tqdm(range(50)):\n",
    "        start_prompt = f\"LYRIC[{genre}]:\"\n",
    "        generated_texts = pipe(start_prompt, max_length=200, truncation= True, num_return_sequences=20)\n",
    "        generated_texts = [generated_text['generated_text'] for generated_text in generated_texts]\n",
    "        generations[genre.lower()] += generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de183737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93faeeb06d0146568e5b349e12c4cd73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53afb1d7aa64e1d8e485ca45f7a6798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366914aaa45c43af8a2f055dcfee3edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69400e1d346406693cf7cda7e39006e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b082367c2948e3b1ac3cb0ce960c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for key in generations:\n",
    "    for i in tqdm.tqdm(range(len(generations[key]))):\n",
    "        generations[key][i] = generations[key][i].split(\"<|endoflyric|>\", 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d84ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# File path to save the dictionary\n",
    "file_path = \"./results/gpt2_med_f.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfc2f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    load_dict = json.load(json_file)\n",
    "lyrics_test_df = pd.read_csv('./data/cleaned_test_lyrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18e0693",
   "metadata": {},
   "source": [
    "# ROGUE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be215831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove text within square brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tqdm.tqdm(tokens) if word.isalnum()]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = \" \".join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "def rouge_precision_recall(reference, candidate, n):\n",
    "    reference_tokens = word_tokenize(reference)\n",
    "    candidate_tokens = word_tokenize(candidate)\n",
    "    \n",
    "    reference_ngrams = list(ngrams(reference_tokens, n))\n",
    "    candidate_ngrams = list(ngrams(candidate_tokens, n))\n",
    "    \n",
    "    # Calculate precision\n",
    "    overlapping_ngrams = set(reference_ngrams) & set(candidate_ngrams)\n",
    "    precision = len(overlapping_ngrams) / len(candidate_ngrams) if len(candidate_ngrams) > 0 else 0\n",
    "    \n",
    "    # Calculate recall\n",
    "    recall = len(overlapping_ngrams)*10 / len(reference_ngrams) if len(reference_ngrams) > 0 else 0\n",
    "    \n",
    "    f1_score = 2*precision*recall/(precision + recall)\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "076e3697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f331b20d0054823822f318c55dcdc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1751623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab74d1623a654fe081cefe56a3f29f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Metal:\n",
      "Rogue1 Precision: 0.028698498663193253\n",
      "Rogue1 Recall: 0.020355321618944003\n",
      "Rogue1 F1: 0.023817397581278914\n",
      "Rogue2 Precision: 0.23104674579030807\n",
      "Rogue2 Recall: 0.16387593761263355\n",
      "Rogue2 F1: 0.19174893562699064\n",
      "Rogue3 Precision: 0.24227466707799886\n",
      "Rogue3 Recall: 0.1718382658630691\n",
      "Rogue3 F1: 0.20106620847391085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9722841564e04156896b98e987fd96fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576c8a85f20441e181085efc57866c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rock:\n",
      "Rogue1 Precision: 0.022286502728030424\n",
      "Rogue1 Recall: 0.01567538649039821\n",
      "Rogue1 F1: 0.018405277027761773\n",
      "Rogue2 Precision: 0.2031854093124895\n",
      "Rogue2 Recall: 0.14291112071186585\n",
      "Rogue2 F1: 0.1677997439333104\n",
      "Rogue3 Precision: 0.2460024514095605\n",
      "Rogue3 Recall: 0.17302550036158848\n",
      "Rogue3 F1: 0.20315922632561353\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c391c76d80f44ec4959a925285418a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4498497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b470ea71af54676b2db48b4f58095a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/166692 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rap:\n",
      "Rogue1 Precision: 0.025587038297107498\n",
      "Rogue1 Recall: 0.009428388010572284\n",
      "Rogue1 F1: 0.013779328173056748\n",
      "Rogue2 Precision: 0.234303284739474\n",
      "Rogue2 Recall: 0.08633623777898727\n",
      "Rogue2 F1: 0.12617823245729343\n",
      "Rogue3 Precision: 0.3237707022724355\n",
      "Rogue3 Recall: 0.11930251657183082\n",
      "Rogue3 F1: 0.17435790713817545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7826b2d0930443a9b513b2e6e961bdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2156776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eccba052b8e94d448b434f2a2409cbc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pop:\n",
      "Rogue1 Precision: 0.021643306079314757\n",
      "Rogue1 Recall: 0.015220275969565315\n",
      "Rogue1 F1: 0.01787222364794303\n",
      "Rogue2 Precision: 0.20136131988236194\n",
      "Rogue2 Recall: 0.1416029065705372\n",
      "Rogue2 F1: 0.1662759318143522\n",
      "Rogue3 Precision: 0.25191545574636726\n",
      "Rogue3 Recall: 0.17715284242476395\n",
      "Rogue3 F1: 0.2080206774838414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386e8f5ee9174ad184a5a972469feb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812cc007732845789828b95c51a1a2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161635 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For country:\n",
      "Rogue1 Precision: 0.020496196515517442\n",
      "Rogue1 Recall: 0.01462782014141148\n",
      "Rogue1 F1: 0.01707177622311424\n",
      "Rogue2 Precision: 0.1995130991818258\n",
      "Rogue2 Recall: 0.14238857573172548\n",
      "Rogue2 F1: 0.16617868888478318\n",
      "Rogue3 Precision: 0.25571498682647104\n",
      "Rogue3 Recall: 0.18249766341788043\n",
      "Rogue3 F1: 0.21298968695104017\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./results/gpt2_med_f.json\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    load_dict = json.load(json_file)\n",
    "\n",
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    reference_text = lyrics_test_df.loc[lyrics_test_df['genre'] == genre, 'Lyric'].tolist()\n",
    "    reference_text = \" \".join(reference_text)\n",
    "\n",
    "    generated_text = load_dict[genre.lower()]\n",
    "    generated_text = \" \".join(generated_text)\n",
    "    reference_text = preprocess_text(reference_text)\n",
    "    generated_text = preprocess_text(generated_text)\n",
    "    \n",
    "    print(f\"For {genre}:\")\n",
    "    for i in range(1,4):\n",
    "        precision, recall, f1_score = rouge_precision_recall(reference_text, generated_text, i)\n",
    "        print(f\"Rogue{i} Precision: {precision}\\nRogue{i} Recall: {recall}\\nRogue{i} F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99056c50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936f968fbde54be98e0be97ef9b7bb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1751623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840fdfae535d43b7b86af9754d3664a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Metal:\n",
      "Rogue1 Precision: 0.02694219878085686\n",
      "Rogue1 Recall: 0.018939140688154527\n",
      "Rogue1 F1: 0.022242685111820176\n",
      "Rogue2 Precision: 0.2186781261348505\n",
      "Rogue2 Recall: 0.15371954373418573\n",
      "Rogue2 F1: 0.18053336255255087\n",
      "Rogue3 Precision: 0.22615064027738147\n",
      "Rogue3 Recall: 0.1589710612495958\n",
      "Rogue3 F1: 0.18670153951141372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2346bb44f6d54108ab83170bd46f5c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb01de8636e4e85b401c883f9848c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rock:\n",
      "Rogue1 Precision: 0.021553653149316433\n",
      "Rogue1 Recall: 0.014709837856797315\n",
      "Rogue1 F1: 0.017485947119357106\n",
      "Rogue2 Precision: 0.19822863206219277\n",
      "Rogue2 Recall: 0.1352852532571272\n",
      "Rogue2 F1: 0.16081735646881112\n",
      "Rogue3 Precision: 0.23434341976092857\n",
      "Rogue3 Recall: 0.15993146568456482\n",
      "Rogue3 F1: 0.1901155157450666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edea024bb189419db75adc4b09e8c06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4498497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9bf141471c48efae2fdcf5e2ab07aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159599 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rap:\n",
      "Rogue1 Precision: 0.020848135575379595\n",
      "Rogue1 Recall: 0.007495875967261307\n",
      "Rogue1 F1: 0.011027023340474744\n",
      "Rogue2 Precision: 0.20302230877022165\n",
      "Rogue2 Recall: 0.0729955150098506\n",
      "Rogue2 F1: 0.10738232614267271\n",
      "Rogue3 Precision: 0.27730556176892407\n",
      "Rogue3 Recall: 0.09970300327178938\n",
      "Rogue3 F1: 0.14667145468881684\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e5b3fade3547088322fe49160d70ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2156776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c73954180145849dfc844cb6ba3b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/146350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pop:\n",
      "Rogue1 Precision: 0.020426560968455094\n",
      "Rogue1 Recall: 0.0139588460947989\n",
      "Rogue1 F1: 0.016584431894621766\n",
      "Rogue2 Precision: 0.19471552859023525\n",
      "Rogue2 Recall: 0.13306135979273426\n",
      "Rogue2 F1: 0.15808993205589233\n",
      "Rogue3 Precision: 0.23768128385098056\n",
      "Rogue3 Recall: 0.16242147835814474\n",
      "Rogue3 F1: 0.19297315163728015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5912dc7b617d4610955bbd06f4aaf8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6069123c4942ad8c2cbede063b6853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157169 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For country:\n",
      "Rogue1 Precision: 0.019015570865357812\n",
      "Rogue1 Recall: 0.013203750694752327\n",
      "Rogue1 F1: 0.015585483794632825\n",
      "Rogue2 Precision: 0.18509275545083465\n",
      "Rogue2 Recall: 0.12852117463201154\n",
      "Rogue2 F1: 0.15170460279065315\n",
      "Rogue3 Precision: 0.23631391706047894\n",
      "Rogue3 Recall: 0.16408613369858366\n",
      "Rogue3 F1: 0.19368547489498092\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./results/gpt2_small_f.json\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    load_dict = json.load(json_file)\n",
    "\n",
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    reference_text = lyrics_test_df.loc[lyrics_test_df['genre'] == genre, 'Lyric'].tolist()\n",
    "    reference_text = \" \".join(reference_text)\n",
    "\n",
    "    generated_text = load_dict[genre.lower()]\n",
    "    generated_text = \" \".join(generated_text)\n",
    "    reference_text = preprocess_text(reference_text)\n",
    "    generated_text = preprocess_text(generated_text)\n",
    "    \n",
    "    print(f\"For {genre}:\")\n",
    "    for i in range(1,4):\n",
    "        precision, recall, f1_score = rouge_precision_recall(reference_text, generated_text, i)\n",
    "        print(f\"Rogue{i} Precision: {precision}\\nRogue{i} Recall: {recall}\\nRogue{i} F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23f5ee90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9d9f4674a94affab7696d5e6d8c968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1751623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9968dc9433a24eccbcf4d00956fa7270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Metal:\n",
      "Rogue1 Precision: 0.0965940445445114\n",
      "Rogue1 Recall: 0.026597458167960275\n",
      "Rogue1 F1: 0.041709955678406324\n",
      "Rogue2 Precision: 0.1476314511500596\n",
      "Rogue2 Recall: 0.04064988764755186\n",
      "Rogue2 F1: 0.06374717686648503\n",
      "Rogue3 Precision: 0.06037394318006225\n",
      "Rogue3 Recall: 0.016623431562517475\n",
      "Rogue3 F1: 0.026068995624030752\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be6cdaa10c842928944ad36233372ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afdbfd154a14a629af0743f3e7f1fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102772 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rock:\n",
      "Rogue1 Precision: 0.08324826566113205\n",
      "Rogue1 Recall: 0.019685369080965194\n",
      "Rogue1 F1: 0.031841347854772856\n",
      "Rogue2 Precision: 0.18\n",
      "Rogue2 Recall: 0.04256298114272737\n",
      "Rogue2 F1: 0.06884645924811539\n",
      "Rogue3 Precision: 0.09533531948582262\n",
      "Rogue3 Recall: 0.022542627043664516\n",
      "Rogue3 F1: 0.036463284516411267\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f64ae5fd7e84a8ea7ca83af1427d84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4498497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c5ac763a8c4949af1e436c1ed54ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110265 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rap:\n",
      "Rogue1 Precision: 0.09980826778002759\n",
      "Rogue1 Recall: 0.013175143306293933\n",
      "Rogue1 F1: 0.023277545234497238\n",
      "Rogue2 Precision: 0.25346736910009676\n",
      "Rogue2 Recall: 0.033458248859829454\n",
      "Rogue2 F1: 0.0591133993088183\n",
      "Rogue3 Precision: 0.14932353731744466\n",
      "Rogue3 Recall: 0.019710686016080777\n",
      "Rogue3 F1: 0.0348245379051701\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c0bbb04e8d4aac9823172dd05f93b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2156776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beca935e952a432883c4af093b1eeaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pop:\n",
      "Rogue1 Precision: 0.09193713547927519\n",
      "Rogue1 Recall: 0.02019265652242363\n",
      "Rogue1 F1: 0.03311260932973825\n",
      "Rogue2 Precision: 0.16522339217737805\n",
      "Rogue2 Recall: 0.03628812832561726\n",
      "Rogue2 F1: 0.059506748227204816\n",
      "Rogue3 Precision: 0.07459928762243989\n",
      "Rogue3 Recall: 0.0163839366037972\n",
      "Rogue3 F1: 0.026867150719023004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6397de02c4e44afa965bda8af48d9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7940e31619dc4815977903fe87375ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110248 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For country:\n",
      "Rogue1 Precision: 0.07484121646865426\n",
      "Rogue1 Recall: 0.02308006404164735\n",
      "Rogue1 F1: 0.03528017730261628\n",
      "Rogue2 Precision: 0.13509878351316615\n",
      "Rogue2 Recall: 0.04166211563357063\n",
      "Rogue2 F1: 0.06368491185381213\n",
      "Rogue3 Precision: 0.062304783823771165\n",
      "Rogue3 Recall: 0.019213433641989533\n",
      "Rogue3 F1: 0.029369848036221637\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./results/gpt2_small_nf.json\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    load_dict = json.load(json_file)\n",
    "\n",
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    reference_text = lyrics_test_df.loc[lyrics_test_df['genre'] == genre, 'Lyric'].tolist()\n",
    "    reference_text = \" \".join(reference_text)\n",
    "\n",
    "    generated_text = load_dict[genre.lower()]\n",
    "    generated_text = \" \".join(generated_text)\n",
    "    reference_text = preprocess_text(reference_text)\n",
    "    generated_text = preprocess_text(generated_text)\n",
    "    \n",
    "    print(f\"For {genre}:\")\n",
    "    for i in range(1,4):\n",
    "        precision, recall, f1_score = rouge_precision_recall(reference_text, generated_text, i)\n",
    "        print(f\"Rogue{i} Precision: {precision}\\nRogue{i} Recall: {recall}\\nRogue{i} F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74f5eef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84466b6a4244484827c649dd01e2f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1751623 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbc2397c0e34f14a460622efa800865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/111938 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Metal:\n",
      "Rogue1 Precision: 0.08531188776698496\n",
      "Rogue1 Recall: 0.037847891227579665\n",
      "Rogue1 F1: 0.052433920797581655\n",
      "Rogue2 Precision: 0.13331963282641457\n",
      "Rogue2 Recall: 0.05914534340585035\n",
      "Rogue2 F1: 0.08193943252038065\n",
      "Rogue3 Precision: 0.05262436805546041\n",
      "Rogue3 Recall: 0.023345740633136974\n",
      "Rogue3 F1: 0.03234311148984615\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8217c764281141bf876cbc2fadaef762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142944 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4f3c4d3740484e83158f0adcf609a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rock:\n",
      "Rogue1 Precision: 0.06936359753493197\n",
      "Rogue1 Recall: 0.028000910374425966\n",
      "Rogue1 F1: 0.03989634250771395\n",
      "Rogue2 Precision: 0.21473201210582837\n",
      "Rogue2 Recall: 0.08668266391058227\n",
      "Rogue2 F1: 0.123507873486553\n",
      "Rogue3 Precision: 0.14190351829930561\n",
      "Rogue3 Recall: 0.05728270700693422\n",
      "Rogue3 F1: 0.08161827103751647\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c96eac7aa304df09b646351a7787508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4498497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bbcb3ba33034f6aac8a4e24e0f92740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rap:\n",
      "Rogue1 Precision: 0.08807341510536011\n",
      "Rogue1 Recall: 0.01854691178898577\n",
      "Rogue1 F1: 0.0306412465332718\n",
      "Rogue2 Precision: 0.27155806665393645\n",
      "Rogue2 Recall: 0.05718533930259716\n",
      "Rogue2 F1: 0.0944757516080241\n",
      "Rogue3 Precision: 0.18114932716285131\n",
      "Rogue3 Recall: 0.03814643386311469\n",
      "Rogue3 F1: 0.06302174556987648\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2180f2be991c4e189bf7bd6efbdefb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2156776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3e8e59bd764c2599a9b8113802a48a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110564 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pop:\n",
      "Rogue1 Precision: 0.07969284700776229\n",
      "Rogue1 Recall: 0.023341341946259177\n",
      "Rogue1 F1: 0.03610719920373567\n",
      "Rogue2 Precision: 0.17566439636752137\n",
      "Rogue2 Recall: 0.05144974055112778\n",
      "Rogue2 F1: 0.07958894800473616\n",
      "Rogue3 Precision: 0.08585546633724521\n",
      "Rogue3 Recall: 0.025145504611557446\n",
      "Rogue3 F1: 0.0388983809106744\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22d1be40f524df7a280406c2ad5b087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2259320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc7bd5eb221846e29686a0a078c496ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/121828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For country:\n",
      "Rogue1 Precision: 0.059396000323860415\n",
      "Rogue1 Recall: 0.01690448780047174\n",
      "Rogue1 F1: 0.02631854625190322\n",
      "Rogue2 Precision: 0.11824335265731774\n",
      "Rogue2 Recall: 0.033652297384550084\n",
      "Rogue2 F1: 0.05239334327577504\n",
      "Rogue3 Precision: 0.05387592505627257\n",
      "Rogue3 Recall: 0.015332956039073921\n",
      "Rogue3 F1: 0.023871999586706587\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./results/gpt2_med_nf.json\"\n",
    "\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    load_dict = json.load(json_file)\n",
    "\n",
    "for genre in lyrics_test_df['genre'].unique():\n",
    "    reference_text = lyrics_test_df.loc[lyrics_test_df['genre'] == genre, 'Lyric'].tolist()\n",
    "    reference_text = \" \".join(reference_text)\n",
    "\n",
    "    generated_text = load_dict[genre.lower()]\n",
    "    generated_text = \" \".join(generated_text)\n",
    "    reference_text = preprocess_text(reference_text)\n",
    "    generated_text = preprocess_text(generated_text)\n",
    "    \n",
    "    print(f\"For {genre}:\")\n",
    "    for i in range(1,4):\n",
    "        precision, recall, f1_score = rouge_precision_recall(reference_text, generated_text, i)\n",
    "        print(f\"Rogue{i} Precision: {precision}\\nRogue{i} Recall: {recall}\\nRogue{i} F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c8a00d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185.7142857142857"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.18 - 0.063)*100/(0.063)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ab09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
